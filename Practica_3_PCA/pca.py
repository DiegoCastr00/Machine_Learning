# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFk0OfCcrYNUvuJrObEz_KfpLfWeKzzP

**Analisis de componentes principales**
> Parte 1
> Visualización de los datos

Solo se gráfica la relación entre 3 de 4 características, sin embargo las características procesadas en las siguientes operaciones si incluyen las 4 características.
"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets import load_iris

# Cargar el conjunto de datos Iris
iris = load_iris()
X = iris.data
feature_names = iris.feature_names

# Indices de las características que se utilizarán en el gráfico 3D
feature1_index = feature_names.index("sepal length (cm)")
feature2_index = feature_names.index("sepal width (cm)")
feature3_index = feature_names.index("petal length (cm)")

# Crear una figura 3D
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Graficar los datos en 3D
ax.scatter(X[:, feature1_index], X[:, feature2_index], X[:, feature3_index], c=iris.target, cmap='viridis', s=50)
ax.set_xlabel(feature_names[feature1_index])
ax.set_ylabel(feature_names[feature2_index])
ax.set_zlabel(feature_names[feature3_index])
ax.set_title('Relación entre tres características de Iris')

plt.show()

"""**Procesamiento de PCA**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Cargar el conjunto de datos Iris
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Estandarizar los datos (opcional pero recomendado para PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Calcular la matriz de covarianza
cov_matrix = np.cov(X_scaled, rowvar=False)

# Calcular los valores propios y vectores propios
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Ordenar los valores propios y vectores propios en orden descendente
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

# Seleccionar el número de componentes principales
num_components = 2  # Cambia esto según cuántos componentes desees

# Tomar los primeros 'num_components' vectores propios
top_eigenvectors = eigenvectors[:, :num_components]

# Proyectar los datos originales en el nuevo espacio de características
X_pca = X_scaled.dot(top_eigenvectors)

# Crear un DataFrame para los componentes principales
pca_df = pd.DataFrame(data=X_pca, columns=['Componente Principal 1', 'Componente Principal 2'])

# Agregar la columna de etiquetas
pca_df['Target'] = y

# Visualizar los resultados
plt.figure(figsize=(10, 6))
targets = np.unique(y)
colors = ['r', 'g', 'b']
for target, color in zip(targets, colors):
    indices_to_keep = pca_df['Target'] == target
    plt.scatter(pca_df.loc[indices_to_keep, 'Componente Principal 1'],
                pca_df.loc[indices_to_keep, 'Componente Principal 2'],
                c=color, label=target)
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend(targets)
plt.title('PCA en el conjunto de datos Iris')
plt.show()

"""**Comparativa datos de dos características sin procesar vs  2 componentes principales (PCA) con respecto a las cuatro características**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Cargar el conjunto de datos Iris
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Estandarizar los datos (opcional pero recomendado para PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Visualizar los datos originales en un gráfico de dispersión en 2D
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for target, color in zip(np.unique(y), ['r', 'g', 'b']):
    indices_to_keep = y == target
    plt.scatter(X[indices_to_keep, 0], X[indices_to_keep, 1], c=color, label=target)
plt.xlabel('Característica 1')
plt.ylabel('Característica 2')
plt.title('Datos Originales')

# Aplicar PCA
cov_matrix = np.cov(X_scaled, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]
num_components = 2
top_eigenvectors = eigenvectors[:, :num_components]
X_pca = X_scaled.dot(top_eigenvectors)

# Visualizar los datos después de aplicar PCA en un gráfico de dispersión en 2D
plt.subplot(1, 2, 2)
for target, color in zip(np.unique(y), ['r', 'g', 'b']):
    indices_to_keep = y == target
    plt.scatter(X_pca[indices_to_keep, 0], X_pca[indices_to_keep, 1], c=color, label=target)
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('Datos después de PCA')

plt.legend()
plt.tight_layout()
plt.show()